{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "import torch\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "# import packages\n",
    "from typing import Dict, List\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "import sys\n",
    "\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.utils.visualizer import ColorMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#easy registering of dataset since it is in COCO format\n",
    "register_coco_instances(\"tdt4265_dataset_train\", {}, \"./tdt4265_2022/train_annotations.json\", \"./tdt4265_2022\") #get the datasets, easy since thet are in coco format\n",
    "register_coco_instances(\"tdt4265_dataset_val\", {}, \"./tdt4265_2022/val_annotations.json\", \"./tdt4265_2022\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/30 19:11:07 d2.data.datasets.coco]: \u001b[0mLoaded 1604 images in COCO format from ./tdt4265_2022/train_annotations.json\n",
      "\u001b[32m[04/30 19:11:07 d2.data.datasets.coco]: \u001b[0mLoaded 301 images in COCO format from ./tdt4265_2022/val_annotations.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#access the data and metadata\n",
    "from detectron2.data import DatasetCatalog\n",
    "\n",
    "#to access the data:\n",
    "trainDataDict: List[Dict] = DatasetCatalog.get(\"tdt4265_dataset_train\")\n",
    "valDataDict: List[Dict] = DatasetCatalog.get(\"tdt4265_dataset_val\")\n",
    "trainMetaData = MetadataCatalog.get(\"tdt4265_dataset_train\")\n",
    "valMetaData = MetadataCatalog.get(\"tdt4265_dataset_val\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/30 19:11:10 d2.data.datasets.coco]: \u001b[0mLoaded 1604 images in COCO format from ./tdt4265_2022/train_annotations.json\n",
      "D IS: {'file_name': './tdt4265_2022\\\\images/train/trip007_glos_Video00012_58.png', 'height': 128, 'width': 1024, 'image_id': 1048, 'annotations': [{'iscrowd': 0, 'bbox': [53.1, 80.42, 68.6, 47.58], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [475.65, 70.86, 19.75, 25.75], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [493.19, 68.21, 9.829999999999984, 12.460000000000008], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [507.46, 73.72, 23.010000000000048, 24.960000000000008], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [496.94, 87.38, 43.660000000000025, 39.84], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [400.01, 71.31, 16.980000000000018, 12.950000000000003], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [10.54, 41.35, 18.96, 40.24999999999999], 'category_id': 1, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [0.0, 74.56, 29.81, 53.06], 'category_id': 0, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}, {'iscrowd': 0, 'bbox': [515.05, 63.19, 3.1800000000000637, 10.510000000000005], 'category_id': 6, 'bbox_mode': <BoxMode.XYWH_ABS: 1>}]}\n",
      "VALUE IS: ./tdt4265_2022\\images/train/trip007_glos_Video00012_58.png\n"
     ]
    }
   ],
   "source": [
    "#to view images from the tdt4265 dataset \n",
    "def showImagesWithAnnotations(numberOfImages : int, dataset : str):\n",
    "    if dataset.lower() == \"train\":\n",
    "        dataDict: List[Dict] = DatasetCatalog.get(\"tdt4265_dataset_train\")\n",
    "    elif dataset.lower() == \"val\":\n",
    "        dataDict: List[Dict] = DatasetCatalog.get(\"tdt4265_dataset_val\")\n",
    "    else:\n",
    "        raise Exception(\"Have to choose either 'train' or 'val'\")\n",
    "\n",
    "    dataset_dicts = dataDict\n",
    "    for d in random.sample(dataset_dicts, numberOfImages):\n",
    "        #print(\"D IS:\", d)\n",
    "        #print(\"VALUE IS:\", d[\"file_name\"])\n",
    "    \n",
    "        img = cv2.imread(d[\"file_name\"])\n",
    "\n",
    "        visualizer = Visualizer(img[:, :, ::-1], metadata=trainMetaData, scale=0.5)\n",
    "        out = visualizer.draw_dataset_dict(d)\n",
    "        cv2.imshow(\"image\", out.get_image()[:, :, ::-1])\n",
    "        cv2.waitKey(0) \n",
    "        #closing all open windows \n",
    "        cv2.destroyAllWindows()  #must have these to view locally in cv2 (in google colab i wouldnt need them)\n",
    "showImagesWithAnnotations(1, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/02 19:06:09 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[05/02 19:06:09 d2.data.datasets.coco]: \u001b[0mLoaded 1604 images in COCO format from ./tdt4265_2022/train_annotations.json\n",
      "\u001b[32m[05/02 19:06:09 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 1604 images left.\n",
      "\u001b[32m[05/02 19:06:09 d2.data.build]: \u001b[0mDistribution of instances among all 8 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|:----------:|:-------------|\n",
      "|    car     | 9563         |   truck    | 123          |    bus     | 445          |\n",
      "| motorcycle | 0            |  bicycle   | 1043         |  scooter   | 615          |\n",
      "|   person   | 4910         |   rider    | 1588         |            |              |\n",
      "|   total    | 18287        |            |              |            |              |\u001b[0m\n",
      "\u001b[32m[05/02 19:06:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[05/02 19:06:09 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[05/02 19:06:09 d2.data.common]: \u001b[0mSerializing 1604 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[05/02 19:06:09 d2.data.common]: \u001b[0mSerialized dataset takes 1.35 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (10, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (10,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (36, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (36,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Train the model by running this cell\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"tdt4265_dataset_train\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 0\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 5e-3 #same lr as in other models  \n",
    "cfg.SOLVER.MAX_ITER = 10000    # same number of steps as original model\n",
    "cfg.SOLVER.STEPS = []        # could decay lr here\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 9  #9 classes which which the tdt4265 dataset has\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train() #if cfg not found, comment out this, and run the cell (so it finds the cfg, but doesnt train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running inference \n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to model\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a iou threshhold to what I used in other models\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[05/02 19:06:15 d2.data.datasets.coco]: \u001b[0mLoaded 4 images in COCO format from ./customDataset/val_annotations.json\n"
     ]
    }
   ],
   "source": [
    "#register custom dataset:\n",
    "\n",
    "register_coco_instances(\"myCustomValDataset\", {}, \"./customDataset/val_annotations.json\", \"./customDataset\") #if you register several times, it will complain, just register once, and then comment it out\n",
    "\n",
    "customValDataDict: List[Dict] = DatasetCatalog.get(\"myCustomValDataset\")\n",
    "customValMetaData = MetadataCatalog.get(\"myCustomValDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ColorMode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\eivin\\NTNU vår 22 lokal\\DDL\\DDL Project\\DETECTRON 2\\detectron2TDT4265.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=7'>8</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=8'>9</a>\u001b[0m outputs \u001b[39m=\u001b[39m predictor(im)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=10'>11</a>\u001b[0m v \u001b[39m=\u001b[39m Visualizer(im[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=11'>12</a>\u001b[0m                 metadata\u001b[39m=\u001b[39mcustomValMetaData, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=12'>13</a>\u001b[0m                 scale\u001b[39m=\u001b[39m\u001b[39m1.2\u001b[39m, \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=13'>14</a>\u001b[0m                 instance_mode\u001b[39m=\u001b[39mColorMode\u001b[39m.\u001b[39mIMAGE_BW   \u001b[39m# remove the colors of unsegmented pixels. This option is only available for segmentation models\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=14'>15</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=15'>16</a>\u001b[0m out \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mdraw_instance_predictions(outputs[\u001b[39m\"\u001b[39m\u001b[39minstances\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/eivin/NTNU%20v%C3%A5r%2022%20lokal/DDL/DDL%20Project/DETECTRON%202/detectron2TDT4265.ipynb#ch0000007?line=17'>18</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m, out\u001b[39m.\u001b[39mget_image()[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ColorMode' is not defined"
     ]
    }
   ],
   "source": [
    "#view custom pictures inferenced by the model\n",
    "\n",
    "for d in random.sample(customValDataDict, 4): \n",
    "    #print(\"D IS:\", d)   \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    if (im is None):\n",
    "        print(\"COULD NOT READ\", d[\"file_name\"])\n",
    "        continue\n",
    "    outputs = predictor(im)\n",
    "\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                    metadata=customValMetaData, \n",
    "                    scale=1.2, \n",
    "                    instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "    cv2.imshow(\"image\", out.get_image()[:, :, ::-1])\n",
    "    cv2.waitKey(0) \n",
    "    #closing all open windows \n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view inferenced images from tdt4265 val dataset:\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "dataset_dicts = valDataDict\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   #metadata=valMetaData, \n",
    "                   scale=1.7, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2.imshow(\"image\", out.get_image()[:, :, ::-1])\n",
    "    cv2.waitKey(0) \n",
    "    #closing all open windows \n",
    "    cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running mAP tests\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "\n",
    "evaluator = COCOEvaluator(\"tdt4265_dataset_val\", output_dir=\"./output\", tasks=(\"bbox\",), distributed=True)\n",
    "val_loader = build_detection_test_loader(cfg, \"tdt4265_dataset_val\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAKA\n"
     ]
    }
   ],
   "source": [
    "#make a inferenced video!\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('./testVideo4.mp4')\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "  raise Exception(\"Error opening video  file\")\n",
    "\n",
    "# Read until video is completed\n",
    "frameArray = []\n",
    "while(cap.isOpened()):\n",
    "      \n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  counter = 0\n",
    "\n",
    "  if ret == True:\n",
    "       \n",
    "        outputs = predictor(frame)  \n",
    "        v = Visualizer(frame[:, :, ::-1],\n",
    "                      metadata=customValMetaData,\n",
    "                      scale=1.7, \n",
    "                      instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "        )\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        frame = out.get_image()[:, :, ::-1]\n",
    "        #cv2.imshow(\"Frame:\", frame)\n",
    "        #if cv2.waitKey(25) & 0xFF == ord('q'): #hold q to exit so you dont have to go through every frame\n",
    "        #  break\n",
    "        #cv2.waitKey(0) \n",
    "        #cv2.destroyAllWindows() \n",
    "\n",
    "        frameArray.append(frame)\n",
    "  else: \n",
    "    print(\"BREAKA\")\n",
    "    break\n",
    "   \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "height,width,layers=frameArray[0].shape\n",
    "video = cv2.VideoWriter('inferencedVideo4.mp4', fourcc, 1, (width, height))\n",
    "\n",
    "\n",
    "for i in range(len(frameArray)):\n",
    "      video.write(frameArray[i])\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'detectron2' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BREAKA\n"
     ]
    }
   ],
   "source": [
    "#inference on custom video:\n",
    "\n",
    "#make a inferenced video!\n",
    "import cv2\n",
    "cap = cv2.VideoCapture('./testVideo3.mp4')\n",
    "\n",
    "if (cap.isOpened()== False): \n",
    "  raise Exception(\"Error opening video  file\")\n",
    "\n",
    "# Read until video is completed\n",
    "frameArray = []\n",
    "while(cap.isOpened()):\n",
    "      \n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  counter = 0\n",
    "\n",
    "  if ret == True:\n",
    "       \n",
    "        outputs = predictor(frame)  \n",
    "        v = Visualizer(frame[:, :, ::-1],\n",
    "                      metadata=customValMetaData,\n",
    "                      scale=1.7, \n",
    "                      instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "        )\n",
    "        out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "        frame = out.get_image()[:, :, ::-1]\n",
    "        #cv2.imshow(\"Frame:\", frame)\n",
    "        #if cv2.waitKey(25) & 0xFF == ord('q'): #hold q to exit so you dont have to go through every frame\n",
    "        #  break\n",
    "        #cv2.waitKey(0) \n",
    "        #cv2.destroyAllWindows() \n",
    "\n",
    "        frameArray.append(frame)\n",
    "  else: \n",
    "    print(\"BREAKA\")\n",
    "    break\n",
    "   \n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "height,width,layers=frameArray[0].shape\n",
    "video = cv2.VideoWriter('inferencedVideo2.mp4', fourcc, 1, (width, height))\n",
    "\n",
    "\n",
    "for i in range(len(frameArray)):\n",
    "      video.write(frameArray[i])\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra stuff\n",
    "\"\"\"\n",
    "#Running inference on custom images. Works pretty well, even works for all image shapes (not just 1024 width, 128 height), but does not show labels\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "\n",
    "#im = cv2.imread(\"./try1img.jpg\")\n",
    "im = cv2.imread(\"./try2img.jpg\")\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:, :, ::-1],\n",
    "                #metadata=valMetaData, \n",
    "                scale=1.2, \n",
    "                instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    ")\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "\n",
    "cv2.imshow(\"image\", out.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0) \n",
    "#closing all open windows \n",
    "cv2.destroyAllWindows() \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71c94d85e77cf4e7097711e9e577fa32bf4dbece2dd5c23284412be120f0e514"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('detectron2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
